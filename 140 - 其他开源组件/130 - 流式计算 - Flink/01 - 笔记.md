# 进度

- [ ] 基础篇：基本概念，quick start，基本运行架构
- [ ] 核心篇：基本 API，时间语义，窗口
- [ ] 高级篇：处理函数，状态编程，容错机制
- [ ] SQL 篇：sql-client，查询 sql，connector，catalog，module




# Flink 概述


是什么：Flink 是一套流式计算框架

- 批处理和流处理用同一套代码或 SQL：不需要分别为批处理和流处理各单独维护一套代码或 SQL
- 性能高：吞吐大，低时延（比较模糊）
- 架构：支持分布式，支持水平扩展，支持存储超大规模的中间计算结果，支持增量检查点机制（不知道这个是啥意思）
- 生态：能集成 Yarn，K8s，或独立运行
- 高容错：自动重试，一致性检查点，保证状态一致性（课上说，大白话意思就是，计算一条数据时出错并重试时，是从头计算还是从出错的那一步开始重试）

## Flink 里一些简单名词和概念的介绍

一些名词的简单解释

- Flink 是事件驱动此应用：Flink 是被动型程序，有数据来就处理，没数据来就歇着
- Flink 的输入和输出源
- 有界流和无界流：无界流好理解，不解释。case：来自 MQ 的数据；有界流相当于知道会来多少数据，全都来后再处理，所以可以等全都来后先排个序啊再处理，case：读取文件里的数据
- 有状态的流处理：如果是无状态的流处理，相当于 Flink 只提供函数和局部变量，如果想记录中间结果就需要引入 MySQL 之类的数据存储组件
- 结果准确性：如果没有明确的事件时间定义，那么接收数据的时间是今天 23:59:59，计算完数据的时间是 00:00:01，那这条数据算是今天计算的还是明天计算的（暂时不知道区分这个有什么意义）
		- 时间时间：数据产生的时间
	- 处理时间：开始处理数据的时间
- 精准一次（exactly-one）：如果处理 MQ 里的消息，做到每条消息只有效消费一次（不丢失消息，不重复计算消息），如果不保证精准一次，就会出现不一致。想要保证数据一直就需要手动签收消息，保证幂等等手段实现 exactly-one。如果用的是 Spark Streaming，exactly-one 需要自行手动写代码实现手动签收，保证幂等等，但 Flink 直接提供了这个功能
- Flink 分层 API：虽然提供了分层 API，但使用时只用 SQL API 就能应对多数情况，也是最易用的；或者用 DataStream API，其它 API 比较少用，因为编程难度相对较大，也不容易理解。Table API 的目的是希望处理数据时像是在处理表中数据一样方便，但封装的的并不好，所以基本不用

> [!tip] Spark 和 Flink 的区别 
> - Spark / Spark Streaming 以批处理为根本，Flink 是真正实现了流处理


## quick start

1. 创建 idea 项目，导入 flink-1.17.0 依赖（flnk-stream-java, flink-clients）
2. 用 data set api 实现 word count